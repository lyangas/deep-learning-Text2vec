{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from future import standard_library\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import wget\n",
    "import re\n",
    "from ufal.udpipe import Model, Pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.cluster import *\n",
    "import word2vec\n",
    "\n",
    "def log_progress(sequence, every=10):\n",
    "    from ipywidgets import IntProgress\n",
    "    from IPython.display import display\n",
    "\n",
    "    progress = IntProgress(min=0, max=len(sequence), value=0)\n",
    "    display(progress)\n",
    "    \n",
    "    for index, record in enumerate(sequence):\n",
    "        if index % every == 0:\n",
    "            progress.value = index\n",
    "        yield record\n",
    "        \n",
    "model = word2vec.load('/Users/ivan/Downloads/182/withOutTypeModel.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(pipeline, text='некий текст', keep_pos=True, keep_punct=False):\n",
    "    wordInfo = pipeline.process(text).split('\\n')[4].split('\\t')\n",
    "    if (wordInfo[3] == 'NUM'):\n",
    "        return ('_NUM_' +'x' * len(word[2]))\n",
    "    else:\n",
    "        return wordInfo[2]\n",
    "\n",
    "def tag_ud(text='некий текст'):\n",
    "    model = Model.load('udpipe_syntagrus.model')\n",
    "    process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "\n",
    "    #print('Processing input...', file=sys.stderr)\n",
    "    return process(process_pipeline, text=text)\n",
    "#print (process(Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu'),\"ксюша\"))\n",
    "#https://github.com/akutuzov/webvectors/blob/master/preprocessing/rusvectores_tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelForInfinitive1 = Model.load('udpipe_syntagrus.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the word is new: а\n",
      "the word is new: и\n",
      "the word is new: Об\n",
      "CPU times: user 209 ms, sys: 11.2 ms, total: 221 ms\n",
      "Wall time: 220 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class wordsClass (object):\n",
    "    model\n",
    "    vectorOfStr = torch.zeros(2000)\n",
    "    centroids = torch.zeros(2000,300)\n",
    "    ClasterOfWord = {}\n",
    "    nomberKnownOfWords = 0\n",
    "    modelForInfinitive = ''\n",
    "\n",
    "    def loadCentroidsFromTXT (self, source):\n",
    "        self.centroids = torch.from_numpy(np.loadtxt(source, delimiter=' ', dtype=float))\n",
    "        self.centroids = torch.reshape(self.centroids, (2000, 300)).cuda()\n",
    "        \n",
    "    def saveCentroidsToTXT (self, source):\n",
    "        f = open (source, 'w')\n",
    "        for i in range (2000):\n",
    "            for j in range (300):\n",
    "                f.write(str(self.centroids[i][j]) + \" \")\n",
    "        f.close()\n",
    "        \n",
    "    def saveDict (self, source):\n",
    "        f = open(source, 'w')\n",
    "        for k, v in self.ClasterOfWord.items():\n",
    "            f.write(str(k) + \" \" + str(v) + \"\\n\")\n",
    "        f.close()\n",
    "            \n",
    "    def loadDict (self, source):\n",
    "        f = open(source, 'r')\n",
    "        for strings in f:\n",
    "            strings = strings.split(' ')\n",
    "            self.ClasterOfWord.update({strings[0]: int(strings[1])})\n",
    "        f.close()\n",
    "        \n",
    "    def generateCentroids (self, nomberOfClasters, NomberOfIter):\n",
    "        #scipy.cluster.vq.kmeans2(model.vectors, 2000, iter=100)\n",
    "        self.centroids = vq.kmeans(self.model.vectors, nomberOfClasters, iter=NomberOfIter)[0]\n",
    "        \n",
    "    def generateDict (self, source): #источник = txt для model\n",
    "        f = open (source, 'r')\n",
    "        n=0\n",
    "        words = []\n",
    "        for line in f:\n",
    "            line = line.split()\n",
    "            words.append(line[0])\n",
    "            n = n+1\n",
    "        print (words[1])\n",
    "        f.close()\n",
    "        for word in log_progress(words, every=100):\n",
    "            distToCentroids = torch.sum(((self.centroids - self.model[word])**2),1)\n",
    "            relationID = int(torch.argmin(distToCentroids, 0))\n",
    "            ClasterOfWord.update({word: relationID})\n",
    "    \n",
    "    def distanceBetween2words (self, word1, word2):\n",
    "        word1 = word1.lower()\n",
    "        word2 = word2.lower()\n",
    "        try:\n",
    "            len = model.distance(tag_ud(text=word1), tag_ud(text=word2))\n",
    "        except Exception:\n",
    "            try:\n",
    "                len =  model.distance(tag_ud(text=word1.capitalize()), tag_ud(text=word2))\n",
    "            except Exception:\n",
    "                try:\n",
    "                    len =  model.distance(tag_ud(text=word1), tag_ud(text=word2.capitalize()))\n",
    "                except Exception:\n",
    "                    len = model.distance(tag_ud(text=word1.capitalize()), tag_ud(text=word2.capitalize()))\n",
    "        return len\n",
    "    \n",
    "    def coordinate (self, word):\n",
    "        word = word.lower()\n",
    "        try:\n",
    "            coord = self.model[tag_ud(text=word)]\n",
    "        except Exception:\n",
    "            try:\n",
    "                coord =  self.model[tag_ud(text=word.capitalize())]\n",
    "            except Exception:\n",
    "                coord = \"error\"\n",
    "        return torch.from_numpy (coord)\n",
    "    \n",
    "    def getKey(self, value):\n",
    "        for k, v in self.ClasterOfWord.items():\n",
    "            if int(v) == value:\n",
    "                print (k)\n",
    "        return k\n",
    "    \n",
    "    def addNewWordInClaster (self, newWord, baseWord):\n",
    "        self.ClasterOfWord.update({newWord: self.ClasterOfWord[baseWord]})\n",
    "    \n",
    "    def tryToUpgradeSelfWord2vec (self, word):\n",
    "        try:\n",
    "            #print (word)\n",
    "            relationID = self.ClasterOfWord[word.lower()]\n",
    "            self.vectorOfStr[relationID] = self.vectorOfStr[relationID] + 1\n",
    "            self.nomberKnownOfWords = self.nomberKnownOfWords + 1\n",
    "            #print (relationID)\n",
    "        except Exception:\n",
    "            if not re.search(r'[\\W]', word):\n",
    "                #print(self.wordToInf(text=word))\n",
    "                relationID = self.ClasterOfWord[self.wordToInf(text=word)]\n",
    "                self.vectorOfStr[relationID] = self.vectorOfStr[relationID] + 1\n",
    "                self.nomberKnownOfWords = self.nomberKnownOfWords + 1\n",
    "                #print (relationID)\n",
    "            else: \n",
    "                raise\n",
    "                \n",
    "            \n",
    "    def text2Vec (self, text):\n",
    "        self.vectorOfStr = torch.zeros(2000)\n",
    "        words = re.findall(r'[0-9A-Za-zА-Яа-я-.]+', text)\n",
    "        for word in words:\n",
    "            try :\n",
    "                self.tryToUpgradeSelfWord2vec(word)\n",
    "            except Exception:\n",
    "                for singleWord in re.findall(r\"[\\w']+\", word):\n",
    "                    try: \n",
    "                        self.tryToUpgradeSelfWord2vec(singleWord)\n",
    "                    except Exception:\n",
    "                        print (\"the word is new: \" + singleWord)\n",
    "        self.vectorOfStr = self.vectorOfStr / self.nomberKnownOfWords    \n",
    "        return self.vectorOfStr\n",
    "    \n",
    "    def wordToInf(self, text):\n",
    "        process_pipeline = Pipeline(self.modelForInfinitive, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "        wordInfo = process_pipeline.process(text).split('\\n')[4].split('\\t')\n",
    "        if (wordInfo[3] == 'NUM'):\n",
    "            return ('_NUM_' + ('x' * len(wordInfo[2])))\n",
    "        else:\n",
    "            return wordInfo[2]\n",
    "        \n",
    "#проработать встречи ::\n",
    "#реализовать работу с реляционной БД (?):\n",
    "#(текст->оценка принадлежности вектор->оценка принадлежности)\n",
    "\n",
    "wordsClass = wordsClass()\n",
    "wordsClass.modelForInfinitive = modelForInfinitive1\n",
    "wordsClass.loadDict('correct_dictionary.txt')\n",
    "#wordsClass.loadDict('correct_dictionary.txt')\n",
    "wordsClass.text2Vec(\"12 3 444 Китайский интернет-гигант Alibaba меняет бизнес-модель торговой площадки AliExpress, дав возможность продавцам из России, а также Турции, Италии и Испании продавать на ней свои товары. Об этом сообщает «Интерфакс».\")\n",
    "#wordsClass.text2Vec(\"12 3 444 вставал\")\n",
    "#wordsClass.ClasterOfWord['Alibaba']\n",
    "#wordsClass.ClasterOfWord.update({'AliExpress': int(1829)})\n",
    "#wordsClass.saveDict('dict words has clasters.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
