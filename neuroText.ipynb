{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from future import standard_library\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import wget\n",
    "import re\n",
    "from ufal.udpipe import Model, Pipeline\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.cluster import *\n",
    "import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(pipeline, text='коты и собаки бегали по грядке', keep_pos=True, keep_punct=False):\n",
    "    entities = {'PROPN'}\n",
    "    named = False\n",
    "    memory = []\n",
    "    mem_case = None\n",
    "    mem_number = None\n",
    "    tagged_propn = []\n",
    "\n",
    "    # обрабатываем текст, получаем результат в формате conllu:\n",
    "    processed = pipeline.process(text)\n",
    "\n",
    "    # пропускаем строки со служебной информацией:\n",
    "    content = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
    "\n",
    "    # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
    "    tagged = [w.split('\\t') for w in content if w]\n",
    "\n",
    "    for t in tagged:\n",
    "        if len(t) != 10:\n",
    "            continue\n",
    "        (word_id, token, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n",
    "        token = clean_token(token, misc)\n",
    "        lemma = clean_lemma(lemma, pos)\n",
    "        if not lemma or not token:\n",
    "            continue\n",
    "        if pos in entities:\n",
    "            if '|' not in feats:\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                continue\n",
    "            morph = {el.split('=')[0]: el.split('=')[1] for el in feats.split('|')}\n",
    "            if 'Case' not in morph or 'Number' not in morph:\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                continue\n",
    "            if not named:\n",
    "                named = True\n",
    "                mem_case = morph['Case']\n",
    "                mem_number = morph['Number']\n",
    "            if morph['Case'] == mem_case and morph['Number'] == mem_number:\n",
    "                memory.append(lemma)\n",
    "                if 'SpacesAfter=\\\\n' in misc or 'SpacesAfter=\\s\\\\n' in misc:\n",
    "                    named = False\n",
    "                    past_lemma = '::'.join(memory)\n",
    "                    memory = []\n",
    "                    tagged_propn.append(past_lemma + '_PROPN ')\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN ')\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "        else:\n",
    "            if not named:\n",
    "                if pos == 'NUM' and token.isdigit():  # Заменяем числа на xxxxx той же длины\n",
    "                    lemma = num_replace(token)\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN ')\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "\n",
    "    if not keep_punct:\n",
    "        tagged_propn = [word for word in tagged_propn if word.split('_')[1] != 'PUNCT']\n",
    "    if not keep_pos:\n",
    "        tagged_propn = [word.split('_')[0] for word in tagged_propn]\n",
    "    return tagged_propn\n",
    "def num_replace(word):\n",
    "    newtoken = 'x' * len(word)\n",
    "    return newtoken\n",
    "\n",
    "\n",
    "def clean_token(token, misc):\n",
    "    \"\"\"\n",
    "    :param token:  токен (строка)\n",
    "    :param misc:  содержимое поля \"MISC\" в CONLLU (строка)\n",
    "    :return: очищенный токен (строка)\n",
    "    \"\"\"\n",
    "    out_token = token.strip().replace(' ', '')\n",
    "    if token == 'Файл' and 'SpaceAfter=No' in misc:\n",
    "        return None\n",
    "    return out_token\n",
    "\n",
    "\n",
    "def clean_lemma(lemma, pos):\n",
    "    \"\"\"\n",
    "    :param lemma: лемма (строка)\n",
    "    :param pos: часть речи (строка)\n",
    "    :return: очищенная лемма (строка)\n",
    "    \"\"\"\n",
    "    out_lemma = lemma.strip().replace(' ', '').replace('_', '').lower()\n",
    "    if '|' in out_lemma or out_lemma.endswith('.jpg') or out_lemma.endswith('.png'):\n",
    "        return None\n",
    "    if pos != 'PUNCT':\n",
    "        if out_lemma.startswith('«') or out_lemma.startswith('»'):\n",
    "            out_lemma = ''.join(out_lemma[1:])\n",
    "        if out_lemma.endswith('«') or out_lemma.endswith('»'):\n",
    "            out_lemma = ''.join(out_lemma[:-1])\n",
    "        if out_lemma.endswith('!') or out_lemma.endswith('?') or out_lemma.endswith(',') \\\n",
    "                or out_lemma.endswith('.'):\n",
    "            out_lemma = ''.join(out_lemma[:-1])\n",
    "    return out_lemma\n",
    "\n",
    "\n",
    "def list_replace(search, replacement, text):\n",
    "    search = [el for el in search if el in text]\n",
    "    for c in search:\n",
    "        text = text.replace(c, replacement)\n",
    "    return text\n",
    "\n",
    "\n",
    "def unify_sym(text):  # принимает строку в юникоде\n",
    "    text = list_replace \\\n",
    "        ('\\u00AB\\u00BB\\u2039\\u203A\\u201E\\u201A\\u201C\\u201F\\u2018\\u201B\\u201D\\u2019', '\\u0022', text)\n",
    "\n",
    "    text = list_replace \\\n",
    "        ('\\u2012\\u2013\\u2014\\u2015\\u203E\\u0305\\u00AF', '\\u2003\\u002D\\u002D\\u2003', text)\n",
    "\n",
    "    text = list_replace('\\u2010\\u2011', '\\u002D', text)\n",
    "\n",
    "    text = list_replace \\\n",
    "            (\n",
    "            '\\u2000\\u2001\\u2002\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200A\\u200B\\u202F\\u205F\\u2060\\u3000',\n",
    "            '\\u2002', text)\n",
    "\n",
    "    text = re.sub('\\u2003\\u2003', '\\u2003', text)\n",
    "    text = re.sub('\\t\\t', '\\t', text)\n",
    "\n",
    "    text = list_replace \\\n",
    "            (\n",
    "            '\\u02CC\\u0307\\u0323\\u2022\\u2023\\u2043\\u204C\\u204D\\u2219\\u25E6\\u00B7\\u00D7\\u22C5\\u2219\\u2062',\n",
    "            '.', text)\n",
    "\n",
    "    text = list_replace('\\u2217', '\\u002A', text)\n",
    "\n",
    "    text = list_replace('…', '...', text)\n",
    "\n",
    "    text = list_replace('\\u2241\\u224B\\u2E2F\\u0483', '\\u223D', text)\n",
    "\n",
    "    text = list_replace('\\u00C4', 'A', text)  # латинская\n",
    "    text = list_replace('\\u00E4', 'a', text)\n",
    "    text = list_replace('\\u00CB', 'E', text)\n",
    "    text = list_replace('\\u00EB', 'e', text)\n",
    "    text = list_replace('\\u1E26', 'H', text)\n",
    "    text = list_replace('\\u1E27', 'h', text)\n",
    "    text = list_replace('\\u00CF', 'I', text)\n",
    "    text = list_replace('\\u00EF', 'i', text)\n",
    "    text = list_replace('\\u00D6', 'O', text)\n",
    "    text = list_replace('\\u00F6', 'o', text)\n",
    "    text = list_replace('\\u00DC', 'U', text)\n",
    "    text = list_replace('\\u00FC', 'u', text)\n",
    "    text = list_replace('\\u0178', 'Y', text)\n",
    "    text = list_replace('\\u00FF', 'y', text)\n",
    "    text = list_replace('\\u00DF', 's', text)\n",
    "    text = list_replace('\\u1E9E', 'S', text)\n",
    "\n",
    "    currencies = list \\\n",
    "            (\n",
    "            '\\u20BD\\u0024\\u00A3\\u20A4\\u20AC\\u20AA\\u2133\\u20BE\\u00A2\\u058F\\u0BF9\\u20BC\\u20A1\\u20A0\\u20B4\\u20A7\\u20B0\\u20BF\\u20A3\\u060B\\u0E3F\\u20A9\\u20B4\\u20B2\\u0192\\u20AB\\u00A5\\u20AD\\u20A1\\u20BA\\u20A6\\u20B1\\uFDFC\\u17DB\\u20B9\\u20A8\\u20B5\\u09F3\\u20B8\\u20AE\\u0192'\n",
    "        )\n",
    "\n",
    "    alphabet = list \\\n",
    "            (\n",
    "            '\\t\\n\\r абвгдеёзжийклмнопрстуфхцчшщьыъэюяАБВГДЕЁЗЖИЙКЛМНОПРСТУФХЦЧШЩЬЫЪЭЮЯ,.[]{}()=+-−*&^%$#@!~;:0123456789§/\\|\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ')\n",
    "\n",
    "    alphabet.append(\"'\")\n",
    "\n",
    "    allowed = set(currencies + alphabet)\n",
    "\n",
    "    cleaned_text = [sym for sym in text if sym in allowed]\n",
    "    cleaned_text = ''.join(cleaned_text)\n",
    "    print (cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def process(pipeline, text='некий текст', keep_pos=True, keep_punct=False):\n",
    "    # Если частеречные тэги не нужны (например, их нет в модели), выставьте pos=False\n",
    "    # в этом случае на выход будут поданы только леммы\n",
    "    # По умолчанию знаки пунктуации вырезаются. Чтобы сохранить их, выставьте punct=True\n",
    "\n",
    "    entities = {'PROPN'}\n",
    "    named = False\n",
    "    memory = []\n",
    "    mem_case = None\n",
    "    mem_number = None\n",
    "    tagged_propn = []\n",
    "\n",
    "    # обрабатываем текст, получаем результат в формате conllu:\n",
    "    processed = pipeline.process(text)\n",
    "    \n",
    "    # пропускаем строки со служебной информацией:\n",
    "    content = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
    "\n",
    "    # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
    "    tagged = [w.split('\\t') for w in content if w]\n",
    "    \n",
    "    for t in tagged:\n",
    "        if len(t) != 10:\n",
    "            continue\n",
    "        (word_id, token, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n",
    "        token = clean_token(token, misc)\n",
    "        lemma = clean_lemma(lemma, pos)\n",
    "        if not lemma or not token:\n",
    "            continue\n",
    "        \n",
    "    \n",
    "    if not keep_punct:\n",
    "        \n",
    "        tagged_propn = [word for word in tagged_propn if word.split('_')[1] != 'PUNCT']\n",
    "    if not keep_pos:\n",
    "        tagged_propn = [word.split('_')[0] for word in tagged_propn]\n",
    "    return lemma\n",
    "def tag_ud(text='некий текст'):\n",
    "    modelfile='udpipe_syntagrus.model'\n",
    "    udpipe_model_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "    udpipe_filename = udpipe_model_url.split('/')[-1]\n",
    "\n",
    "    if not os.path.isfile(modelfile):\n",
    "        print('UDPipe model not found. Downloading...', file=sys.stderr)\n",
    "        wget.download(udpipe_model_url)\n",
    "\n",
    "    #print('\\nLoading the model...', file=sys.stderr)\n",
    "    model = Model.load(modelfile)\n",
    "    process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "\n",
    "    #print('Processing input...', file=sys.stderr)\n",
    "    return process(process_pipeline, text=text)\n",
    "#print (process(Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu'),\"ксюша\"))\n",
    "#https://github.com/akutuzov/webvectors/blob/master/preprocessing/rusvectores_tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.load('/Users/ivan/Downloads/182/withOutTypeModel.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 300])\n",
      "1\n",
      "2\n",
      "3\n",
      "1454\n",
      "1417\n",
      "1432\n",
      "1454\n",
      "1454\n",
      "1144\n",
      "325\n",
      "1173\n",
      "391\n",
      "1169\n",
      "CPU times: user 27.3 s, sys: 1.22 s, total: 28.5 s\n",
      "Wall time: 28.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class wordsClass (object):\n",
    "    model\n",
    "    vectorOfStr = torch.zeros(2000)\n",
    "    centroids\n",
    "    \n",
    "    nomberKnownOfWords = 0\n",
    "    def loadCentroidsFromTXT (self, source):\n",
    "        self.centroids = torch.from_numpy(np.loadtxt(source, delimiter=' ', dtype=float))\n",
    "        self.centroids = torch.reshape(self.centroids, (2000, 300)).cuda()\n",
    "        \n",
    "    def saveCentroidsToTXT (self, source):\n",
    "        f = open (source, 'w')\n",
    "        for i in range (2000):\n",
    "            for j in range (300):\n",
    "                f.write(str(self.centroids[i][j]) + \" \")\n",
    "        f.close()\n",
    "        \n",
    "    def generateCentroids (self, nomberOfClasters, NomberOfIter):\n",
    "        #scipy.cluster.vq.kmeans2(model.vectors, 2000, iter=100)\n",
    "        self.centroids = vq.kmeans(self.model.vectors, nomberOfClasters, iter=NomberOfIter)[0]\n",
    "        \n",
    "    def textToVec (self, text):\n",
    "        distToCentroids = torch.zeros(2000).cuda()\n",
    "        print (1)\n",
    "        self.centroids.cuda()\n",
    "        print (2)\n",
    "        words = re.findall(r'[A-Za-zА-Яа-я]+', text)\n",
    "        print (3)\n",
    "        for word in words:\n",
    "            #try:\n",
    "            coordOfTheWord = self.coordinate(word).cuda()\n",
    "            delta = self.centroids.cuda() - coordOfTheWord.cuda()\n",
    "            distToCentroids = torch.sum((delta.mul_(delta).cuda()),1).cuda()\n",
    "            relationID = int(torch.argmin(distToCentroids, 0))\n",
    "            self.nomberKnownOfWords = self.nomberKnownOfWords + 1\n",
    "            self.vectorOfStr[relationID] = self.vectorOfStr[relationID] + 1\n",
    "            print (relationID)\n",
    "                \n",
    "            coordOfTheWord.cpu()\n",
    "            #except Exception:\n",
    "                #relationID = \"none\"     \n",
    "                #print (\"the word is new: \" + word)\n",
    "        self.vectorOfStr = self.vectorOfStr / self.nomberKnownOfWords\n",
    "        \n",
    "        self.centroids.cpu\n",
    "        distToCentroids.cpu\n",
    "        return 0\n",
    "    \n",
    "    def torchDistance (self, vec1, vec2):\n",
    "        var = np.sum(((vec1-vec2)**2),1)\n",
    "        return var\n",
    "        \n",
    "    \n",
    "    def distance (self, word1, word2):\n",
    "        word1 = word1.lower()\n",
    "        word2 = word2.lower()\n",
    "        try:\n",
    "            len = model.distance(tag_ud(text=word1), tag_ud(text=word2))\n",
    "        except Exception:\n",
    "            try:\n",
    "                len =  model.distance(tag_ud(text=word1.capitalize()), tag_ud(text=word2))\n",
    "            except Exception:\n",
    "                try:\n",
    "                    len =  model.distance(tag_ud(text=word1), tag_ud(text=word2.capitalize()))\n",
    "                except Exception:\n",
    "                    len = model.distance(tag_ud(text=word1.capitalize()), tag_ud(text=word2.capitalize()))\n",
    "        return len\n",
    "    \n",
    "    \n",
    "    def coordinate (self, word):\n",
    "        word = word.lower()\n",
    "        try:\n",
    "            coord = self.model[tag_ud(text=word)]\n",
    "        except Exception:\n",
    "            try:\n",
    "                coord =  self.model[tag_ud(text=word.capitalize())]\n",
    "            except Exception:\n",
    "                coord = \"error\"\n",
    "        return torch.from_numpy (coord)\n",
    "\n",
    "wordsClass = wordsClass()\n",
    "wordsClass.model = model\n",
    "wordsClass.loadCentroidsFromTXT ('2000centroids-400.txt')\n",
    "print (wordsClass.centroids.shape)\n",
    "#wordsClass.textToVec(\"серый синий красный черный \", centroids)\n",
    "wordsClass.textToVec(\"гитара пианино бас-гитара электрогитара скрипка кот кошка собака акула\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3000)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsClass.vectorOfStr[1454]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "год\n"
     ]
    }
   ],
   "source": [
    "f = open ('/Users/ivan/Downloads/182/withOutTypeModel.txt', 'r')\n",
    "n=0\n",
    "words = []#array(24978)\n",
    "for line in f:\n",
    "    line = line.split()\n",
    "    words.append(line[0])\n",
    "    n = n+1\n",
    "print (words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(sequence, every=10):\n",
    "    from ipywidgets import IntProgress\n",
    "    from IPython.display import display\n",
    "\n",
    "    progress = IntProgress(min=0, max=len(sequence), value=0)\n",
    "    display(progress)\n",
    "    \n",
    "    for index, record in enumerate(sequence):\n",
    "        if index % every == 0:\n",
    "            progress.value = index\n",
    "        yield record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load is complite\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7eb54ddd0c449438710297e7fabf364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=248978)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "centroids = np.loadtxt('2000centroids-400.txt', delimiter=' ', dtype=float)\n",
    "centroids = np.reshape(centroids, (2000, 300))\n",
    "print(\"load is complite\")\n",
    "ClasterOfWord = {}\n",
    "for word in log_progress(words, every=100):\n",
    "    distToCentroids = np.sum(((centroids - model[word])**2),1)\n",
    "    relationID = int(np.argmin(distToCentroids, 0))\n",
    "    ClasterOfWord.update({word: relationID})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ClasterOfWord['электрогитара']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "звуковой\n",
      "акустический\n",
      "клавиша\n",
      "звукозапись\n",
      "гитарный\n",
      "усилитель\n",
      "громкость\n",
      "громкоговоритель\n",
      "fender\n",
      "магнитофонный\n",
      "midi\n",
      "электрогитара\n",
      "шумовой\n",
      "граммофонный\n",
      "микширование\n",
      "звукосниматель\n",
      "стереофонический\n",
      "шумовый\n",
      "драм-машина\n",
      "электроакустический\n",
      "семплер\n",
      "мануал\n",
      "реверберация\n",
      "микрофонный\n",
      "stratocaster\n",
      "монофонический\n",
      "ibanez\n",
      "секвенсор\n",
      "усилитеть\n",
      "сэмпл\n",
      "драм-машин\n",
      "тремоло\n",
      "telecaster\n",
      "-машина\n",
      "эквалайзер\n",
      "хамбакер\n",
      "тюнер\n",
      "moog\n",
      "emg\n",
      "диджейский\n",
      "микшер\n",
      "вокодер\n",
      "акустически\n",
      "электроорган\n",
      "барабать\n",
      "электрогитарин\n",
      "электромузыкальный\n",
      "дисторшн\n",
      "ревербератор\n",
      "микшерный\n",
      "soloist\n",
      "семплирование\n",
      "струнодержатель\n",
      "предусилитель\n",
      "rickenbacker\n",
      "электрофон\n",
      "лид\n",
      "семпло\n",
      "трактура\n",
      "шумоподавление\n",
      "резонаторный\n",
      "бинауральный\n",
      "тэппинг\n",
      "бас-гитарин\n",
      "исполнения\n",
      "полуакустический\n",
      "-машин\n",
      "lfo\n",
      "овердрайв\n",
      "хай-хэт\n",
      "звука\n",
      "-барабан\n"
     ]
    }
   ],
   "source": [
    "def get_key(d, value):\n",
    "    for k, v in d.items():\n",
    "        if v == value:\n",
    "            print (k)\n",
    "get_key(ClasterOfWord, 428)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248978, 300)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file /Users/ivan/Downloads/182/model.bin\n",
      "Vocab size: 2512\n",
      "Words in train file: 1039877\n",
      "Alpha: 0.000161  Progress: 99.68%  Words/thread/sec: 36.48k  "
     ]
    }
   ],
   "source": [
    "word2vec.word2clusters('/Users/ivan/Downloads/182/model.bin', '100word2vecClast.txt', 2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xbe in position 14: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-e62c62d0f43a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'100word2vecClast.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_words_on_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/word2vec/io.py\u001b[0m in \u001b[0;36mload_clusters\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mLoads\u001b[0m \u001b[0ma\u001b[0m \u001b[0mword\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \"\"\"\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordClusters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/word2vec/wordclusters.py\u001b[0m in \u001b[0;36mfrom_text\u001b[0;34m(cls, fname)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding)\u001b[0m\n\u001b[1;32m   1722\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m             \u001b[0mfirst_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_decode_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfhd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcomments\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfirst_line\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xbe in position 14: invalid start byte"
     ]
    }
   ],
   "source": [
    "clusters = word2vec.load_clusters('100word2vecClast.txt')\n",
    "clusters.get_words_on_cluster(0)[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torchDistance (vec1, vec2, dim):\n",
    "    vec1.cuda()\n",
    "    vec2.cuda()\n",
    "    vec1 = vec1-vec2\n",
    "    vec1 = vec1.mul_(vec1)\n",
    "    vec1 = torch.sum(vec1, dim)\n",
    "    \n",
    "    return torch.sqrt(vec1).to(torch.double)\n",
    "\n",
    "def myCentroids (vectors, Nclastersm, NOfIters):\n",
    "    Nvectors = vectors.shape[0]\n",
    "    dimenshonsOfVec = vectors.shape[1]\n",
    "    distToCentroids = torch.rand(Nclasters, Nvectors).cuda()\n",
    "    relation = torch.rand(Nvectors).cuda()\n",
    "    distance = torch.rand(Nvectors).cuda()\n",
    "    centroids = torch.rand(Nclasters, dimenshonsOfVec).cuda()\n",
    "    print (centroids[0][0])\n",
    "    \n",
    "    for NOfIter in range (NOfIters):\n",
    "        print (\"iter N = \" + str(NOfIter))\n",
    "\n",
    "        for i in range (Nclasters):\n",
    "            distToCentroids[i] = torchDistance(centroids[i], vectors, 1)\n",
    "            #подсчет расстояния от каждой центроиды до каджого вектора\n",
    "\n",
    "        relation = torch.argmin(distToCentroids, 0)\n",
    "        #какой кластер владеет данным вектором\n",
    "\n",
    "        for i in range (Nclasters):\n",
    "            sumForCentroids = torch.zeros(dimenshonsOfVec).cuda()\n",
    "            nForCentroid = 0\n",
    "            for j in range (Nvectors):\n",
    "                if (relation[j] == i):\n",
    "                    sumForCentroids = sumForCentroids + vectors[j]\n",
    "                    nForCentroid = nForCentroid + 1\n",
    "            if (nForCentroid != 0): \n",
    "                centroids[i] = sumForCentroids/nForCentroid\n",
    "        #пересчет центроид как центр масс своих чекторов  \n",
    "        print (centroids[0][0])\n",
    "        \n",
    "    sumForCentroids.cpu()\n",
    "    distToCentroids.cpu()\n",
    "    relation.cpu()\n",
    "    distance.cpu()\n",
    "    centroids.cpu()\n",
    "\n",
    "    return centroids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
