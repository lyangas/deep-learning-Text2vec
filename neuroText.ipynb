{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from future import standard_library\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import wget\n",
    "import re\n",
    "from ufal.udpipe import Model, Pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.cluster import *\n",
    "import word2vec\n",
    "\n",
    "def log_progress(sequence, every=10):\n",
    "    from ipywidgets import IntProgress\n",
    "    from IPython.display import display\n",
    "\n",
    "    progress = IntProgress(min=0, max=len(sequence), value=0)\n",
    "    display(progress)\n",
    "    \n",
    "    for index, record in enumerate(sequence):\n",
    "        if index % every == 0:\n",
    "            progress.value = index\n",
    "        yield record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEmodelForInfinitive = Model.load('udpipe_syntagrus.model')\n",
    "#model = word2vec.load('dataForNeurotext/withOutTypeModel.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "CPU times: user 208 ms, sys: 11.5 ms, total: 220 ms\n",
      "Wall time: 220 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class wordsClass (object):\n",
    "    model = ''\n",
    "    vectorOfStr = np.zeros(2000)\n",
    "    centroids = torch.zeros(2000,300)\n",
    "    ClasterOfWord = {}\n",
    "    nomberKnownOfWords = 0\n",
    "    nomberUNKnownOfWords = 0\n",
    "    modelForInfinitive = ''\n",
    "\n",
    "    def loadCentroidsFromTXT (self, source):\n",
    "        self.centroids = torch.from_numpy(np.loadtxt(source, delimiter=' ', dtype=float))\n",
    "        self.centroids = torch.reshape(self.centroids, (2000, 300)).cuda()\n",
    "        \n",
    "    def saveCentroidsToTXT (self, source):\n",
    "        f = open (source, 'w')\n",
    "        for i in range (2000):\n",
    "            for j in range (300):\n",
    "                f.write(str(self.centroids[i][j]) + \" \")\n",
    "        f.close()\n",
    "        \n",
    "    def saveDict (self, source):\n",
    "        f = open(source, 'w')\n",
    "        for k, v in self.ClasterOfWord.items():\n",
    "            f.write(str(k) + \" \" + str(v) + \"\\n\")\n",
    "        f.close()\n",
    "            \n",
    "    def loadDict (self, source):\n",
    "        f = open(source, 'r')\n",
    "        for strings in f:\n",
    "            strings = strings.split(' ')\n",
    "            self.ClasterOfWord.update({strings[0]: int(strings[1])})\n",
    "        f.close()\n",
    "        \n",
    "    def generateCentroids (self, nomberOfClasters, NomberOfIter):\n",
    "        #scipy.cluster.vq.kmeans2(model.vectors, 2000, iter=100)\n",
    "        self.centroids = vq.kmeans(self.model.vectors, nomberOfClasters, iter=NomberOfIter)[0]\n",
    "        \n",
    "    def generateDict (self, source): #источник = txt для model\n",
    "        f = open (source, 'r')\n",
    "        n=0\n",
    "        words = []\n",
    "        for line in f:\n",
    "            line = line.split()\n",
    "            words.append(line[0])\n",
    "            n = n+1\n",
    "        print (words[1])\n",
    "        f.close()\n",
    "        for word in log_progress(words, every=100):\n",
    "            distToCentroids = torch.sum(((self.centroids - self.model[word])**2),1)\n",
    "            relationID = int(torch.argmin(distToCentroids, 0))\n",
    "            ClasterOfWord.update({word: relationID})\n",
    "    \n",
    "    def distanceBetween2words (self, word1, word2):\n",
    "        word1 = word1.lower()\n",
    "        word2 = word2.lower()\n",
    "        try:\n",
    "            len = model.distance(tag_ud(text=word1), tag_ud(text=word2))\n",
    "        except Exception:\n",
    "            try:\n",
    "                len =  model.distance(tag_ud(text=word1.capitalize()), tag_ud(text=word2))\n",
    "            except Exception:\n",
    "                try:\n",
    "                    len =  model.distance(tag_ud(text=word1), tag_ud(text=word2.capitalize()))\n",
    "                except Exception:\n",
    "                    len = model.distance(tag_ud(text=word1.capitalize()), tag_ud(text=word2.capitalize()))\n",
    "        return len\n",
    "    \n",
    "    def coordinate (self, word):\n",
    "        word = word.lower()\n",
    "        try:\n",
    "            coord = self.model[tag_ud(text=word)]\n",
    "        except Exception:\n",
    "            try:\n",
    "                coord =  self.model[tag_ud(text=word.capitalize())]\n",
    "            except Exception:\n",
    "                coord = \"error\"\n",
    "        return torch.from_numpy (coord)\n",
    "    \n",
    "    def getKey(self, value):\n",
    "        for k, v in self.ClasterOfWord.items():\n",
    "            if int(v) == value:\n",
    "                print (k)\n",
    "        return k\n",
    "    \n",
    "    def addNewWordInClaster (self, newWord, baseWord):\n",
    "        self.ClasterOfWord.update({newWord: self.ClasterOfWord[baseWord]})\n",
    "    \n",
    "    def tryToUpgradeSelfWord2vec (self, word):\n",
    "        try:\n",
    "            #print (word)\n",
    "            relationID = self.ClasterOfWord[word.lower()]\n",
    "            self.vectorOfStr[relationID] = self.vectorOfStr[relationID] + 1\n",
    "            self.nomberKnownOfWords = self.nomberKnownOfWords + 1\n",
    "            #print (relationID)\n",
    "        except Exception:\n",
    "            if not re.search(r'[\\W]', word):\n",
    "                #print(self.wordToInf(text=word))\n",
    "                relationID = self.ClasterOfWord[self.wordToInf(text=word)]\n",
    "                self.vectorOfStr[relationID] = self.vectorOfStr[relationID] + 1\n",
    "                self.nomberKnownOfWords = self.nomberKnownOfWords + 1\n",
    "                #print (relationID)\n",
    "            else: \n",
    "                raise\n",
    "                \n",
    "            \n",
    "    def text2Vec (self, text):\n",
    "        self.vectorOfStr = np.zeros(2000)\n",
    "        words = re.findall(r'[0-9A-Za-zА-Яа-я-.]+', text)\n",
    "        for word in words:\n",
    "            try :\n",
    "                self.tryToUpgradeSelfWord2vec(word)\n",
    "            except Exception:\n",
    "                for singleWord in re.findall(r\"[\\w']+\", word):\n",
    "                    try: \n",
    "                        self.tryToUpgradeSelfWord2vec(singleWord)\n",
    "                    except Exception:\n",
    "                        self.nomberUNKnownOfWords = self.nomberUNKnownOfWords + 1\n",
    "                        #print (\"the word is new: \" + singleWord)\n",
    "        self.vectorOfStr = self.vectorOfStr / self.nomberKnownOfWords    \n",
    "        return self.vectorOfStr\n",
    "    \n",
    "    def wordToInf(self, text):\n",
    "        process_pipeline = Pipeline(self.modelForInfinitive, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "        wordInfo = process_pipeline.process(text).split('\\n')[4].split('\\t')\n",
    "        if (wordInfo[3] == 'NUM'):\n",
    "            return ('_NUM_' + ('x' * len(wordInfo[2])))\n",
    "        else:\n",
    "            return wordInfo[2]\n",
    "        \n",
    "#проработать встречи ::\n",
    "#реализовать работу с реляционной БД (?):\n",
    "#(текст->оценка принадлежности вектор->оценка принадлежности)\n",
    "#поддержка новый слов через сервис\n",
    "\n",
    "wordsClass = wordsClass()\n",
    "wordsClass.modelForInfinitive = FILEmodelForInfinitive\n",
    "wordsClass.loadDict('correct_dictionary.txt')\n",
    "\n",
    "#wordsClass.text2Vec(\"рaссказывать\")\n",
    "#wordsClass.ClasterOfWord['Alibaba']\n",
    "#wordsClass.saveDict('dict words has clasters.txt')\n",
    "\n",
    "#wordsClass.text2Vec(\"Китайский интернет-гигант Alibaba меняет бизнес-модель торговой площадки AliExpress, дав возможность продавцам из России, а также Турции, Италии и Испании продавать на ней свои товары. Об этом сообщает «Интерфакс».\")\n",
    "print (wordsClass.vectorOfStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "42\n",
      "63\n",
      "84\n",
      "105\n",
      "126\n",
      "147\n",
      "168\n",
      "189\n",
      "210\n",
      "231\n",
      "252\n",
      "273\n",
      "294\n",
      "315\n",
      "336\n",
      "357\n",
      "378\n",
      "399\n",
      "420\n",
      "441\n",
      "462\n",
      "483\n",
      "504\n",
      "525\n",
      "546\n",
      "567\n",
      "588\n",
      "609\n",
      "630\n",
      "651\n",
      "672\n",
      "693\n",
      "714\n",
      "735\n",
      "756\n",
      "777\n",
      "798\n",
      "819\n",
      "840\n",
      "861\n",
      "882\n",
      "903\n",
      "924\n",
      "945\n",
      "966\n",
      "987\n",
      "1008\n",
      "1029\n",
      "1050\n",
      "1071\n",
      "1092\n",
      "1113\n",
      "1134\n",
      "1155\n",
      "1176\n",
      "1197\n",
      "1218\n",
      "1239\n",
      "1260\n",
      "1281\n",
      "1302\n",
      "1323\n",
      "1344\n",
      "1365\n",
      "1386\n",
      "1407\n",
      "1428\n",
      "1449\n",
      "1470\n",
      "1491\n",
      "1512\n",
      "1533\n",
      "1554\n",
      "1575\n",
      "1596\n",
      "1617\n",
      "1638\n",
      "1659\n",
      "1680\n",
      "1701\n",
      "1722\n",
      "1743\n",
      "1764\n",
      "1785\n",
      "1806\n",
      "1827\n",
      "1848\n",
      "1869\n",
      "1890\n",
      "1911\n",
      "1932\n",
      "1953\n",
      "1974\n",
      "1995\n",
      "2016\n",
      "2037\n",
      "2058\n",
      "2079\n",
      "2100\n",
      "2121\n",
      "2142\n",
      "2163\n",
      "2184\n",
      "2205\n",
      "2226\n",
      "2247\n",
      "2268\n",
      "2289\n",
      "2310\n",
      "2331\n",
      "2352\n",
      "2373\n",
      "2394\n",
      "2415\n",
      "2436\n",
      "2457\n",
      "2478\n",
      "2499\n",
      "2520\n",
      "2541\n",
      "2562\n",
      "2583\n",
      "2604\n",
      "2625\n",
      "2646\n",
      "2667\n",
      "2688\n",
      "2709\n",
      "2730\n",
      "2751\n",
      "2772\n",
      "2793\n",
      "2814\n",
      "2835\n",
      "2856\n",
      "2877\n",
      "2898\n",
      "2919\n",
      "2940\n",
      "2961\n",
      "2982\n",
      "3003\n",
      "3024\n",
      "3045\n",
      "3066\n",
      "3087\n",
      "3108\n",
      "3129\n",
      "3150\n",
      "3171\n",
      "3192\n",
      "3213\n",
      "3234\n",
      "3255\n",
      "3276\n",
      "3297\n",
      "3318\n",
      "3339\n",
      "3360\n",
      "3381\n",
      "3402\n",
      "3423\n",
      "3444\n",
      "3465\n",
      "3486\n",
      "3507\n",
      "3528\n",
      "3549\n",
      "3570\n",
      "3591\n",
      "3612\n",
      "3633\n",
      "3654\n",
      "3675\n",
      "3696\n",
      "3717\n",
      "3738\n",
      "3759\n",
      "3780\n",
      "3801\n",
      "3822\n",
      "3843\n",
      "3864\n",
      "3885\n",
      "3906\n",
      "3927\n",
      "3948\n",
      "3969\n",
      "3990\n",
      "4011\n",
      "понято слов: 5235375884\n",
      "не понято слов: 851772535\n",
      "всего текстов обработано: 4022\n",
      "CPU times: user 2min 16s, sys: 313 ms, total: 2min 17s\n",
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#обрабатываем все данные\n",
    "Ntext = 0\n",
    "NKnownWods = 0\n",
    "NUNKnownWords = 0\n",
    "f = open ('dataWithText.txt', 'r')\n",
    "fout = open ('data.txt' , 'w')\n",
    "relationToClasses = np.zeros(42)\n",
    "\n",
    "sumOfVectors = np.zeros(2000)\n",
    "\n",
    "NForVisualization = 0\n",
    "\n",
    "for strin in f:\n",
    "    strs = strin.split('~')\n",
    "    \n",
    "    vecOfText = wordsClass.text2Vec(strs[1])\n",
    "    #получили вектор текста\n",
    "    \n",
    "    text = ''\n",
    "    for k in range (2000):\n",
    "        text = text + str(vecOfText[k])\n",
    "        if k != 1999:\n",
    "            text = text + '~'\n",
    "    #сформировали строку-вектор текста\n",
    "    \n",
    "    for l in range (2, 44):\n",
    "        relationToClasses[l-2] = float (strs[l])\n",
    "    #загрузили вектор принадлежности данного текста\n",
    "    \n",
    "    vecToStr = ''\n",
    "    for k in range (42):\n",
    "        vecToStr = vecToStr + str(relationToClasses[k])\n",
    "        if k != 41:\n",
    "            vecToStr = vecToStr + '~'\n",
    "    #сформировали строку-вектор принадлежности\n",
    "    \n",
    "    fout.write(text + '|' + vecToStr + '\\n')\n",
    "    #записали\n",
    "    \n",
    "    sumOfVectors = sumOfVectors + vecOfText\n",
    "    NKnownWods = NKnownWods + wordsClass.nomberKnownOfWords\n",
    "    NUNKnownWords = NUNKnownWords + wordsClass.nomberUNKnownOfWords\n",
    "    Ntext = Ntext + 1\n",
    "    \n",
    "    if (Ntext > NForVisualization + 20):\n",
    "        NForVisualization = Ntext\n",
    "        print (NForVisualization) \n",
    "    \n",
    "f.close()\n",
    "fout.close()\n",
    "print ('понято слов: ' + str(NKnownWods))\n",
    "print ('не понято слов: ' + str(NUNKnownWords))\n",
    "print ('всего текстов обработано: ' + str(Ntext))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
